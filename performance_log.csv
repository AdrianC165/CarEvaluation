Activation, Learning Rate, Epochs, Hidden layers, Training Accuracy, Training Error, Test Accuracy, Test Error,
"Activation=logistic, LR=0.01, Epochs=100, Layers=2",0.8223938223938224,0.17760617760617758,0.8179190751445087,0.18208092485549132
"Activation=logistic, LR=0.01, Epochs=100, Layers=3",0.832046332046332,0.16795366795366795,0.815028901734104,0.18497109826589597
"Activation=logistic, LR=0.01, Epochs=200, Layers=2",0.833976833976834,0.16602316602316602,0.8236994219653179,0.17630057803468213
"Activation=logistic, LR=0.01, Epochs=200, Layers=3",0.8455598455598455,0.15444015444015446,0.8280346820809249,0.1719653179190751
"Activation=logistic, LR=0.1, Epochs=100, Layers=2",0.8532818532818532,0.14671814671814676,0.8280346820809249,0.1719653179190751
"Activation=logistic, LR=0.1, Epochs=100, Layers=3",0.917953667953668,0.08204633204633205,0.8916184971098265,0.10838150289017345
"Activation=logistic, LR=0.1, Epochs=200, Layers=2",0.8774131274131274,0.12258687258687262,0.8728323699421965,0.12716763005780352
"Activation=logistic, LR=0.1, Epochs=200, Layers=3",0.9111969111969112,0.08880308880308885,0.9002890173410405,0.0997109826589595
"Activation=tanh, LR=0.01, Epochs=100, Layers=2",0.7847490347490348,0.2152509652509652,0.7861271676300579,0.21387283236994215
"Activation=tanh, LR=0.01, Epochs=100, Layers=3",0.8783783783783784,0.1216216216216216,0.880057803468208,0.11994219653179194
"Activation=tanh, LR=0.01, Epochs=200, Layers=2",0.7847490347490348,0.2152509652509652,0.7846820809248555,0.21531791907514453
"Activation=tanh, LR=0.01, Epochs=200, Layers=3",0.8832046332046332,0.11679536679536684,0.880057803468208,0.11994219653179194
"Activation=tanh, LR=0.1, Epochs=100, Layers=2",0.8397683397683398,0.16023166023166024,0.8280346820809249,0.1719653179190751
"Activation=tanh, LR=0.1, Epochs=100, Layers=3",0.8243243243243243,0.17567567567567566,0.8208092485549133,0.17919075144508667
"Activation=tanh, LR=0.1, Epochs=200, Layers=2",0.694015444015444,0.305984555984556,0.7095375722543352,0.2904624277456648
"Activation=tanh, LR=0.1, Epochs=200, Layers=3",0.8716216216216216,0.1283783783783784,0.8554913294797688,0.1445086705202312
"Activation=relu, LR=0.01, Epochs=100, Layers=2",0.8021235521235521,0.19787644787644787,0.7933526011560693,0.20664739884393069
"Activation=relu, LR=0.01, Epochs=100, Layers=3",0.8378378378378378,0.16216216216216217,0.8164739884393064,0.1835260115606936
"Activation=relu, LR=0.01, Epochs=200, Layers=2",0.8416988416988417,0.15830115830115832,0.8251445086705202,0.17485549132947975
"Activation=relu, LR=0.01, Epochs=200, Layers=3",0.9227799227799228,0.07722007722007718,0.911849710982659,0.08815028901734101
"Activation=relu, LR=0.1, Epochs=100, Layers=2",0.7934362934362934,0.2065637065637066,0.7976878612716763,0.20231213872832365
"Activation=relu, LR=0.1, Epochs=100, Layers=3",0.7953667953667953,0.20463320463320467,0.7947976878612717,0.2052023121387283
"Activation=relu, LR=0.1, Epochs=200, Layers=2",0.8581081081081081,0.14189189189189189,0.8352601156069365,0.16473988439306353
"Activation=relu, LR=0.1, Epochs=200, Layers=3",0.8378378378378378,0.16216216216216217,0.8106936416184971,0.1893063583815029
------------------------------------------------------------------------------------
"Activation=logistic, LR=0.01, Epochs=100, Layers=2",0.811,0.189,0.824,0.176
"Activation=logistic, LR=0.01, Epochs=100, Layers=3",0.813,0.187,0.821,0.179
"Activation=logistic, LR=0.01, Epochs=200, Layers=2",0.822,0.178,0.829,0.171
"Activation=logistic, LR=0.01, Epochs=200, Layers=3",0.873,0.127,0.877,0.123
"Activation=logistic, LR=0.1, Epochs=100, Layers=2",0.866,0.134,0.863,0.137
"Activation=logistic, LR=0.1, Epochs=100, Layers=3",0.875,0.125,0.873,0.127
"Activation=logistic, LR=0.1, Epochs=200, Layers=2",0.83,0.17,0.827,0.173
"Activation=logistic, LR=0.1, Epochs=200, Layers=3",0.914,0.086,0.906,0.094
"Activation=tanh, LR=0.01, Epochs=100, Layers=2",0.691,0.309,0.714,0.286
"Activation=tanh, LR=0.01, Epochs=100, Layers=3",0.849,0.151,0.848,0.152
"Activation=tanh, LR=0.01, Epochs=200, Layers=2",0.824,0.176,0.827,0.173
"Activation=tanh, LR=0.01, Epochs=200, Layers=3",0.889,0.111,0.883,0.117
"Activation=tanh, LR=0.1, Epochs=100, Layers=2",0.691,0.309,0.714,0.286
"Activation=tanh, LR=0.1, Epochs=100, Layers=3",0.832,0.168,0.827,0.173
"Activation=tanh, LR=0.1, Epochs=200, Layers=2",0.79,0.21,0.795,0.205
"Activation=tanh, LR=0.1, Epochs=200, Layers=3",0.864,0.136,0.864,0.136
"Activation=relu, LR=0.01, Epochs=100, Layers=2",0.824,0.176,0.825,0.175
"Activation=relu, LR=0.01, Epochs=100, Layers=3",0.831,0.169,0.824,0.176
"Activation=relu, LR=0.01, Epochs=200, Layers=2",0.879,0.121,0.877,0.123
"Activation=relu, LR=0.01, Epochs=200, Layers=3",0.882,0.118,0.877,0.123
"Activation=relu, LR=0.1, Epochs=100, Layers=2",0.799,0.201,0.788,0.212
"Activation=relu, LR=0.1, Epochs=100, Layers=3",0.799,0.201,0.786,0.214
"Activation=relu, LR=0.1, Epochs=200, Layers=2",0.786,0.214,0.776,0.224
"Activation=relu, LR=0.1, Epochs=200, Layers=3",0.934,0.066,0.932,0.068
------------------------------------------------------------------------------------
"Activation=logistic, LR=0.01, Epochs=100, Layers=2",0.83,0.17,0.772,0.228
"Activation=logistic, LR=0.01, Epochs=100, Layers=3",0.822,0.178,0.805,0.195
"Activation=logistic, LR=0.01, Epochs=200, Layers=2",0.86,0.14,0.815,0.185
"Activation=logistic, LR=0.01, Epochs=200, Layers=3",0.906,0.094,0.866,0.134
"Activation=logistic, LR=0.1, Epochs=100, Layers=2",0.844,0.156,0.801,0.199
"Activation=logistic, LR=0.1, Epochs=100, Layers=3",0.913,0.087,0.868,0.132
"Activation=logistic, LR=0.1, Epochs=200, Layers=2",0.852,0.148,0.822,0.178
"Activation=logistic, LR=0.1, Epochs=200, Layers=3",0.918,0.082,0.88,0.12
"Activation=tanh, LR=0.01, Epochs=100, Layers=2",0.847,0.153,0.811,0.189
"Activation=tanh, LR=0.01, Epochs=100, Layers=3",0.798,0.202,0.754,0.246
"Activation=tanh, LR=0.01, Epochs=200, Layers=2",0.798,0.202,0.76,0.24
"Activation=tanh, LR=0.01, Epochs=200, Layers=3",0.863,0.137,0.822,0.178
"Activation=tanh, LR=0.1, Epochs=100, Layers=2",0.704,0.296,0.695,0.305
"Activation=tanh, LR=0.1, Epochs=100, Layers=3",0.843,0.157,0.837,0.163
"Activation=tanh, LR=0.1, Epochs=200, Layers=2",0.798,0.202,0.79,0.21
"Activation=tanh, LR=0.1, Epochs=200, Layers=3",0.895,0.105,0.861,0.139
"Activation=relu, LR=0.01, Epochs=100, Layers=2",0.704,0.296,0.695,0.305
"Activation=relu, LR=0.01, Epochs=100, Layers=3",0.842,0.158,0.829,0.171
"Activation=relu, LR=0.01, Epochs=200, Layers=2",0.818,0.182,0.78,0.22
"Activation=relu, LR=0.01, Epochs=200, Layers=3",0.897,0.103,0.868,0.132
"Activation=relu, LR=0.1, Epochs=100, Layers=2",0.889,0.111,0.848,0.152
"Activation=relu, LR=0.1, Epochs=100, Layers=3",0.796,0.204,0.741,0.259
"Activation=relu, LR=0.1, Epochs=200, Layers=2",0.794,0.206,0.76,0.24
"Activation=relu, LR=0.1, Epochs=200, Layers=3",0.794,0.206,0.741,0.259
------------------------------------------------------------------------------------
"Activation=logistic, LR=0.01, Epochs=100, Layers=2",0.779,0.221,0.78,0.22
"Activation=logistic, LR=0.01, Epochs=100, Layers=3",0.836,0.164,0.832,0.168
"Activation=logistic, LR=0.01, Epochs=200, Layers=2",0.833,0.167,0.821,0.179
"Activation=logistic, LR=0.01, Epochs=200, Layers=3",0.846,0.154,0.825,0.175
"Activation=logistic, LR=0.1, Epochs=100, Layers=2",0.867,0.133,0.84,0.16
"Activation=logistic, LR=0.1, Epochs=100, Layers=3",0.879,0.121,0.874,0.126
"Activation=logistic, LR=0.1, Epochs=200, Layers=2",0.842,0.158,0.838,0.162
"Activation=logistic, LR=0.1, Epochs=200, Layers=3",0.906,0.094,0.887,0.113
"Activation=tanh, LR=0.01, Epochs=100, Layers=2",0.816,0.184,0.828,0.172
"Activation=tanh, LR=0.01, Epochs=100, Layers=3",0.831,0.169,0.828,0.172
"Activation=tanh, LR=0.01, Epochs=200, Layers=2",0.828,0.172,0.832,0.168
"Activation=tanh, LR=0.01, Epochs=200, Layers=3",0.827,0.173,0.822,0.178
"Activation=tanh, LR=0.1, Epochs=100, Layers=2",0.783,0.217,0.782,0.218
"Activation=tanh, LR=0.1, Epochs=100, Layers=3",0.836,0.164,0.827,0.173
"Activation=tanh, LR=0.1, Epochs=200, Layers=2",0.786,0.214,0.777,0.223
"Activation=tanh, LR=0.1, Epochs=200, Layers=3",0.821,0.179,0.805,0.195
"Activation=relu, LR=0.01, Epochs=100, Layers=2",0.829,0.171,0.829,0.171
"Activation=relu, LR=0.01, Epochs=100, Layers=3",0.806,0.194,0.808,0.192
"Activation=relu, LR=0.01, Epochs=200, Layers=2",0.892,0.108,0.871,0.129
"Activation=relu, LR=0.01, Epochs=200, Layers=3",0.83,0.17,0.828,0.172
"Activation=relu, LR=0.1, Epochs=100, Layers=2",0.847,0.153,0.84,0.16
"Activation=relu, LR=0.1, Epochs=100, Layers=3",0.799,0.201,0.783,0.217
"Activation=relu, LR=0.1, Epochs=200, Layers=2",0.779,0.221,0.789,0.211
"Activation=relu, LR=0.1, Epochs=200, Layers=3",0.896,0.104,0.87,0.13
